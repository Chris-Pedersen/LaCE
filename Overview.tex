\section{Cosmological analyses with the \lya\ forest power spectrum}
\label{sec:over}

In this section we present an overview of the different aspects involved in 
a cosmological analysis of the small scale clustering from the \lya\ forest
power spectrum.

\begin{itemize}
 \item Measurement of the flux power spectrum: calibrate the quasar spectra, 
  fit the quasar continua, measure 2-point functions, covariance matrices 
  and possible contaminants.
 \item \textit{Emulate} the flux power spectrum: using hydrodynamical 
  simulations, build an emulator to translate the measured flux power to 
  constraints on the linear power spectrum of density at the redshift of 
  the measurement ($z \sim 3$).
 \item Cosmological constraints: combine the inferred linear power spectrum
  with external datasets (primarily CMB studies) to constraint the parameters
  of a particular cosmological model.
\end{itemize}


\subsection{Measuring the \lya\ correlations}

Skip over continuuming fitting, instrumental details, and so on.
This will not be the main focus of this paper.

\begin{figure}[h]
 \begin{center}
  \includegraphics[scale=0.34]{Figures/Walther2018_P1D}
  %\includegraphics[scale=0.6]{Figures/WaltherP1D}
 \end{center}
 \caption{Compilation of $P_{1D}(z,\kpar)$ measurements, including those from
  \cite{Viel2013} (orange), \cite{Irsic2017} (red), 
  \cite{Palanque-Delabrouille2013} (green) and \cite{Walther2018a} (blue). 
  \AFR{This figure was stolen from \cite{Walther2018b}, it would be better to 
  add one that goes to lower values of $\kpar$ covered by SDSS/BOSS.}
 }
 \label{fig:dataP1D}
\end{figure}

Beyond BAO analyses \cite{Bautista2017,duMasdesBourboux2017}, the only 
public results on the clustering of the \lya\ forest are measurements of 
the 1D power spectrum \cite{Croft1998,McDonald2000,McDonald2006} or 
more recently \cite{Viel2013,Palanque-Delabrouille2013,Irsic2017,Walther2018a}.
These are usually presented as band powers measured in different redshift
bins, spanning the redshift $1.8 < z < 5.4$, and covering a broad range of 
scales $0.001 \ikms < \kpar < 0.1 \ikms$. 
A compilation of recent measurements is shown in Figure \ref{fig:dataP1D}.

In most cases, the line of sight wavenumbers are expressed in units of 
inverse velocity, $\ikms$, but other mappings from observed wavelength 
could be used.
When measuring the 3D power, we will also need to define transverse 
wavenumbers, and in this case the natural choice is to use units of 
inverse angles \cite{Font-Ribera2018}.


\subsection{\textit{Emulating} the flux power spectrum}

In order to interpret the measurements, we need to be able to make 
theoretical predictions for the measured correlations, and setup a statistical 
method to compare models and compute constraints on their parameters.  

Due to its non-linear nature, to simulate the \lya\ forest (or flux) power 
spectrum we need to rely on expensive hydrodynamical simulations. 
Each model evaluation requires thousands of CPU hours, what makes a 
\textit{brute-force} approach unfeasible. 

Moreover, the flux power spectrum does not only depend on the density 
clustering, but it also depends on the thermal history of the 
Inter-Galactic Medium (IGM) (see \cite{Walther2018b} for a recent review).
In order to compute robust cosmological constraints, we need to make sure 
that we explore all possible thermal histories, and that we fully marginalize 
over these nuisance parameters.

This large parameter space, and the cost of a single run, makes it impossible 
to run a simulation for each model we want to compare.
A small number of simulations is usually available, and in the past different 
analysis have used different techniques to circumvent this problem: 
a smooth dependence was assumed and fit in \cite{McDonald2005a}; 
a Taylor expansion was used in \cite{Palanque-Delabrouille2013}; 
a Gaussian Process-based \textit{emulator} was used in 
\cite{Walther2018a,Walther2018b}, although in this analysis the cosmological
model was kept fixed and only the thermal history was explored.

Emulators based on Gaussian Processes are an active area of research 
\AFR{cite density emulators}, and it is possible that they will have an 
important role in future \lya\ forest analyses.
However, in this publication we will use the term \textit{emulator} in a broad
sense, to describe any setup to use a finite suite of hydrodynamical 
simulations to make predictions for the flux power spectrum.

The main topic of this paper is to study the parameter degeneracies in this 
type of analyses, and discuss how this might affect the parameterization of 
an emulator for the flux power spectrum.


\subsection{Cosmological constraints}

As we will discuss in the next sections, once we have marginalized over 
the thermal history of the IGM, the flux power spectrum is mostly 
sensitive to the amplitude and slope of the linear power spectrum around 
$k \sim 1 \iMpc$ 
\footnote{As it will be discussed in the next sections, the pivot point 
should be specified in velocity units, $k \sim 0.01 \ikms$.} 
and around $z \sim 3$.
For instance, \cite{McDonald2005a} measured these quantities with 15\% and 
5\% precision respectively.

Changes in other (traditional) cosmological parameters are very degenerate 
with changes in the linear power or in the thermal history. 
For instance, as discussed in \cite{Viel2010} and more recently in 
\cite{Pedersen2018}, at fixed linear power spectrum the effect of massive 
neutrinos is almost indistinguishable from that of $\Omega_m$. 
Another exmaple: at fixed linear power spectrum, and fixed $\Omega_m$, the 
effect of $\Omega_b$ is degenerate with the level of UV background assumed.

One of the peculiarities of \lya\ forest analyses that we will discuss is 
that it covers a redshift range $2 < z < 5$ where the universe was close 
to Einsteint-de Sitter (EdS), i.e., $\Omega_m(z) \sim 1$. 
This means that the growth of structure in this redshift range is almost  
independent of cosmology, and so is the expansion history $H(z)/H_0$.

It is only in combination with external datasets, mainly from CMB experiments,
that we are able to provide constraints on the traditional cosmological 
parameters. 
For instance, by combining our measurement at $k \sim 1 \iMpc$ with the CMB 
constraints on the amplitude and slope of the linear power spectrum at 
$k \sim 0.05 \iMpc$ we get tight constraints on the running of the spectral
index ($\alpha_s$) and on the sum of the neutrino masses ($\sum m_\nu$). 

This approach of emulating only the linear power was used in 
\cite{McDonald2005a}, but recent analyses 
\cite{Palanque-Delabrouille2015,Yeche2017} have attempted instead to 
directly emulate the 6+ traditional parameters of a $\Lambda$-CDM universe. 
We will argue that this might not be the best approach, because of the
following reasons:
\begin{itemize}
 \item As discussed above, the traditional parameters have strong degeneracies
  and this makes the emulator (or Taylor expansion) more challenging. 
 \item In order to break the degeneracies, one needs to adopt priors from 
  CMB experiments, making it more dificult to combine with CMB experiments 
  later without double counting information.
 \item The results are model-dependent, and it is impossible to use them in 
  extended models (curved universe, non-$\Lambda$, different $N_{\rm eff}$...)
  if these were not in the original emulator.
\end{itemize}


\section{Public likelihood code}

In order to properly discuss our choice of parameterization and simulation
setup, it is important to describe how we intend the end product,
the \textit{likelihood code}, to be used in a cosmological analysis.


\subsection{User interface}

The aim of the project is to provide a public likelihood package that others
can use to include \lya\ results in their cosmological parameter constraints.
The end-user does not need to know about the internal details, about the 
suite of simulations, the nuisance parameters or the interpolation scheme 
used in the emulator.
They do not need to know either whether internally we are describing the 
power in units of $\kms$, $\Mpc$ or $\hMpc$. 

There are different possible interfaces that we could setup, and probably 
we will want to provide more than one with different levels of complexity.
But we will start discussing a particular interface, where we will ask 
the user to provide for each cosmologicla model:
\begin{itemize}
 \item Linear matter power (baryons+CDM), as a function of redshift and 
  wavenumber, in units of $\iMpc$. 
  The redshift range should cover at least $2 < z < 5$, and the wavenumber
  range should cover at least $0.01 \iMpc < k < 10 \iMpc$. 
  For simplicity, we will restrict ourselves to models with a linear power 
  spectra that can be factorized ,in the range of redshift and scales 
  described above, in a power spectrum at a central redshift, 
  $\Plin(z_\star=3,k)$, and a scale independent growth factor, $D(z)$.
 \item Logarithmic growth rate, $f(z)$, that could be computed directly
  from the evolution of $D(z)$.
 \item Hubble parameter as a function of redshift, $H(z)$, over the same 
  redshift range.
 \item If we want the likelihood code to be able to fit 3D clustering as well,
  we would need as an input the angular diameter distance, $D_A(z)$, 
  over the full redshift range.
\end{itemize}

The user will also specify:
\begin{itemize}
 \item Data products to use: 
  SDSS-I from \cite{McDonald2006}, 
  BOSS from \cite{Palanque-Delabrouille2013},
  HIRES/UVES from \cite{Viel2013},
  XQSO-100 from \cite{Irsic2017}, 
  HIRES/UVES from \cite{Walther2018a}. 
  We should also allow the user to specify what redshifts to use from each
  dataset, since they are independent, and probably also allow the user 
  to specify the scales to be used in the fit for each dataset.
 \item Extra analysis settings, like whether to allow for running in the 
  linear power, non-EdS linear growth...
 \item \AFR{It is not clear to me whether at this point the user would also
  be able to set other settings of the likelihood or not, like the way we 
  treat contamination by DLAs or metals, resolution or noise corrections, 
  or the parameterization of the temperature-density relation in the 
  simulations.}
 \item \AFR{I guess the user should also be able to specify priors for
  the parameters that we want to marginalize over. 
  We could even provide the option to use published temperature / mean flux
  results as a prior on thermal history and mean flux, although this should 
  be an option that would be switched off by default...}
\end{itemize}

The output from the \textit{likelihood code} will be:
\begin{itemize}
 \item A value of (log-)likelihood for each dataset, possibly a value for 
  each redshift bin. 
  Most users will only care about this.
 \item For the experts, we would also output the best-fit theoretical 
  prediction for each dataset, for that particular cosmological model.
  We would also provide the values of the nuisance parameters that correspond
  to the best fit model (mean flux, temperature-density relation, metal or DLA
  contamination...).
 \item We could also provide a random sample of theory lines that are above a 
  likelihood threshold for that particular model, exploring different 
  thermal histories and other nuisance parameters.
\end{itemize}


\subsection{From cosmological model to emulator input}

Under the hood, we will use more effective (and cryptic) parameters in our 
\textit{emulator}, understood as the package that will use black magic 
(or white magic) and a suite of hydrodynamical simulations to make 
predictions for the flux power spectrum.

There are many, many possibilities here, but we will start by discussing a 
possible setting. 

\begin{itemize}
 \item We will choose a fiducial cosmological model, based on a recent 
  Planck+BAO analysis, and use it to compute a fiducial linear power spectrum,
  $\Plin^0(z,k)$, a fiducial Hubble expansion, $H^0(z)$, a fiducial growth 
  rate $f^0(z)$...
 \item We will compress the difference between the input power spectrum, 
  and the fiducial power spectrum, into a handful of parameters. 
  Probably this would include the sound horizon at the drag epoch, setting 
  the scale of the Baryon Acoustic Oscillations (BAO), $r_d$, and 3 extra 
  parameters describing the smooth differences in the broadband.
  We would ignore differences between baryons and CDM, and always work with 
  their weighted-averaged power. 
  We will fit these parameters using the linear power at a central redshfit,
  $z_\star=3$, but since we have decided to use only models where the 
  linear power spectrum can be factorized, this should not have an impact.
 \item We will compute the ratio of the input growth rate with that in the 
  fiducial cosmology, $R_f = f(z_\star)/f^0(z_\star)$, evaluated at 
  $z_\star=3$. 
  We will approximate that the growth rate at $z_\star$ is enough to compute 
  the linear power at any redshift within the range:
  \begin{equation}
   \Plin(z_\star+\Delta z,k) = \Plin(z_\star,k) \left[ 1 
      - 2 \frac{f(z_\star)}{(1+z_\star)} \right] ~. 
  \end{equation} 
  \AFR{I haven't cross-checked the equation above, but you got the idea.}
\end{itemize}

If we could observe the \lya\ power spectrum in comoving coordinates, that 
would be enough. 
However, we observe the power spectrum in observing coordinates, wavelengths
and angles, and a more natural choice is to use velocity units ($\kms$) for 
the clustering measurements. 
Indeed, all recent measurements of the 1D \lya\ power reported their results
in units of $\kms$, and we will assume the same in this discussion.




