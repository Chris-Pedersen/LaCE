\section{Public likelihood code} \label{sec:like}

In order to properly discuss our choice of parameterization and simulation
setup, it will be helpful to start by describing how we intend the end
product, the \textit{likelihood code}, to be used in a cosmological analysis.


\subsection{User interface}

\ashid{I don't like the expression ``user interface'' here. Wouldn't
  likelihood parameterization be more appropriate?}
\afhid{\anze, this is an internal document, to make sure we are all (including
Chris, Hiranya, Andrew, Tom...) in the same page.
The second part of this section is about the parameterization, this first
part is really just about the interface...
As Pat mentiones below, we can give an easy interface to the users,
and under the hood we compress that into any set of parameters we want.}

The aim of the project is to provide a public likelihood package that others
can use to include \lya\ results in their cosmological parameter constraints.
The end-user does not need to know about the internal details, about the
suite of simulations, the nuisance parameters or the interpolation scheme
used in the emulator.
They do not need to know either whether internally we are describing the
power in units of $\kms$, $\Mpc$ or $\hMpc$, etc.

There are different possible interfaces that we could setup, and probably
we will want to provide more than one with different levels of complexity.
But we will start by discussing a particular interface, where we will ask
the user to provide for each cosmological model:
\begin{itemize}
 \item $P(z,k)$, the linear density power spectrum
  \footnote{During the rest of this paper we will use linear power spectrum
  to refer to the baryons+CDM power.}, as a function of redshift and
  wavenumber, in units of $\iMpc$.
  The redshift range should cover at least $2 < z < 5$, and the wavenumber
  range should cover at least $0.01 \iMpc < k < 10 \iMpc$.
 \item $P_\theta(z,k)$, the linear power spectrum of velocity divergence,
  or \textit{velocity power}, over the same range of redshift and scales.
  Equivalently we could ask for $f(z,k)$, since one could compute one from
  the other.
 \item Hubble parameter as a function of redshift, $H(z)$, over the same
  redshift range.
 \item If we wanted the likelihood to be able to describe 3D clustering as well,
  we would need as an input the angular diameter distance $D_A(z)$ (or the
  Alcock-Paczy\'nski coefficient, $\propto D_A(z) H(z)$, \cite{Alcock1979})
  over the full redshift range.
  \afhid{as well as the sound horizon at the drag epoch describing BAO, $r_d$. 
  Although we could also fit $r_d$ from the linear power ourselves, under the
  hood, to prevent having some users using EH98 and others using CAMB.}
  \pvmhid{Yes, I don't understand why you talk about asking for this here,
   when you have no plan to do anything with it... it just seems like noise...}
\end{itemize}
 
In exchange, the user will get a value of the (log-) likelihood for this model,
that can be then combined with other cosmological probes.

\afhid{For simplicity, we will restrict ourselves to models with a linear power
  spectra that can be factorized in a power spectrum at a central redshift,
  $P(z_\star=3,k)$, and a scale independent growth factor, $D(z)$.}
\ashid{If you are to compress flux measurement over X redshift bins
  into one linear power constrain, just ask for the linear power
  at that $z_{\rm pivot}$ and $f$ at that redshift. If you are going
  to compress into two $z$s, ask for those two $z$s and two $f$s}.
\afhid{Yes, this is an option. But as Pat mentions below, this might not
 be the easier option for some users. But when I mentioned above that we
 could have more than one interface, this is what I had in mind.}

\pvmhid{I don't think you should skimp on what you want from the outside. E.g., 
in neutrino mass era it is starting to feel a little quaint to talk 
about $D(z)$, 
$f(z)$ -- both generally depend on $k$. At the same time, producing density 
and velocity power spectra as a function of k and z is just getting easier and 
easier as people get more comfortable with things like CLASS. 
(I mean, no I 
don't actually think $k$ dependence of $D$ and $f$ will ever matter, 
but it seems
just easier to ask for the linear $P(z,k)$ than worry about defining and
justifying these things
(e.g., I remember kind of
laughing at Alexey Makarov for producing $D(z)$ by running CMBfast, which 
seemed like overkill when I had a little numerical integrator code, but to him
this was actually easier than setting up that code...))
I wouldn't worry
too much about what you're going to ask for though. Focus on making a good 
emulator and then ask for what you need for that...}
\afhid{Great, that's all I wanted to hear. For now, let's assume this is a 
possible option, and we'll come back to it later on.} 
\ashid{I like Pat's idea of just asking for a bunch of power spectra,
including matter, velocity and perhaps baryon as well. For example,
you could easily also check if the power spectra are not so
pathological that approximations employed are not valid. For example
Pat's code wasn't the right thing to use for WDM models and here you
can guard against that. }


\subsection{Full likelihood vs look-up table}

We will consider two type of users: the experts, who will like to access
the full likelihood, and the non-experts that will like to have a quick and
easy access to the marginalized likelihood, i.e., where simulation details
have been hidden away and astrophysical / nuisance parameters have already been
marginalized over.

As an example, in the SDSS-I analysis of \cite{McDonald2005a} the authors
made public a look-up table with the tabulated value of the marginalized
likelihood as a function of three parameters: the amplitude, the slope, and
the running of the linear power spectrum at $z=3$ and $k=0.009\ikms$.

In the next sections we will discuss slightly different parameterizations,
but we will still have the goal of publishing a simple and fast look-up table
that can be easily pluged in \textit{CosmoMC}, \textit{MontePhyton}, etc.
We will refer to this object as the \textit{compressed likelihood}, the
\textit{look-up table}, or the \textit{marginalized likelihood}, and we will
use $\theta$ to refer to the parameters in this final product.

However, in order to compute this compressed likelihood, we will first need
to marginalize the \textit{full likelihood} over the nuisance astrophysical
parameters, that we will refer as $\phi$, and that include mean flux,
thermal history, contaminants in the data, etc.
Both likelihoods are related by:
\begin{equation} \label{eq:marg}
 L(\vd | \theta)
  = \int d\phi ~ \Pi(\phi) ~ L(\vd | \theta, \phi) ~,
\end{equation}
where $\vd$ is the measured flux power spectrum, and $\Pi(\phi)$ are the
priors on the nuisance parameters.

We will do this marginalization ourselves, and published the already
marginalized likelihood for everyone else to use.

The emulator discussed later in this paper is only used to compute the
full likelihood, but once we have the marginalized likelihood we do not
need to use it any more.
In other words, most users will not even need to install the emulator code,
they will directly use the pre-computed look-up table.

We could make available different look-up tables for different combinations
of \lya\ measurements:
SDSS-I from \cite{McDonald2006},
BOSS from \cite{Palanque-Delabrouille2013},
HIRES/UVES from \cite{Viel2013},
XQSO-100 from \cite{Irsic2017} and
HIRES/UVES from \cite{Walther2018a}.
Since these measurements share many nuisance parameters (mean flux,
thermal history...) it will be important to do the marginalization
at the same time.

Of course, we will still make public the code that can compute the full
likelihood $L(\vd | \theta, \phi)$.
This will be important to allow external experts to cross-check our results,
and to test whether the results depend on the compression step.
It will also be important to allow others to generate new look-up tables
by using different priors on the nuisance parameters $\Pi(\phi)$, or by adding
new datasets $\vd$.

Those accessing the full likelihood will be able to ask also for the value
of the (log-) likelihood for a particular data point, or a particular
redshift.
They will be able to run their own MCMC chain and compute the best fit 
values for the nuisance parameters, and their confidence intervals.

\afhid{Actually, I don't think at this point you can get any of the above.
The user of the final likelihood code will only get a total value for the
likelihood for the total dataset.
Even though one would compute the un-marginalized likelihood for each redshift
and each dataset separately, at the step of marginalizing over nuisance 
parameters (see section \ref{sec:emu}) you would need to include them 
all at the same time.}
\ashid{No, no. You don't marginalise over nuissance parameters inside
  your likelihood code. Some nuissance parameters will have particular
  degeneracies with the big picture cosmological parameters, and in
  addition you never know how clever the user's sampler is. It is ok
  to ask for linear power, $f$, $H(z)$, $D_A(z)$ and a set of
  nuissance parameters, which users doesn't need to understand except
  for valid ranges. You can add a small wrapper that does this
  marginalisation internally for those who don't care.}
\pvmhid{In principle I more or less agree with \anze\ about the nuisance 
parameters, the core level of the code should be written treating all 
parameters symmetrically and this should be kept accessible... although,
if you were, e.g., importance sampling a Planck chain you would effectively
return to needing the code to marginalize over nuisance parameters. On the 
other hand, I remember long ago in Alexey Makarov days we ended up retreating
from this idea because the generic MCMC was painfully slowed by marginalization
over these nuisances when we knew how to make it fast internally... Ugh - 
I'm going to try not to read anything but emulator section since I think this
discussion of final public likelihood code is getting way ahead of yourself... 
\smiley\
I'd focus on ``what do I need to do to get an emulator capable of any kind of
fit to a LyaF data set?" ...  }

\afhid{Nooooooo! I need to understand how the final package will be used, 
before I can think of emulators and simulations!}

\afhid{Pat, in your 2005 paper, you had $\chi^2$ look-up tables, that were only
a function of amplitude and slope (and may be running). 
I always thougth that this type of look-up table would be the end product of 
the analysis, with a nice wrapper around it with a public interface to 
translate cosmology to our cryptic parameters in the lookup table.
Isn't it the case? 
You made the look-up table public, and that table had already marginalized
over all nuisance parameters, right?}
\pvmhid{Yes. This is getting at what I rambled about in comment later. While
\anze's desired option to control all parameters simultaneously in a cosmoMC
run should certainly *exist,* I don't think you should present this as, e.g., 
what anyone would run in their first use of your results. I do think you still
want to produce some form of ``de-forested'' linear power main result, that
will contain everything most people would want. You need it for 
``opening the black box" purposes, but also, it will be faster, and why force
people to mess with a bunch of forest nuisance parameters they don't care 
about when you know they can get the same results without it? The option to
run global chain with LyaF nuisances should exist for comparison, and so you
can see correlations, but not be the lead option... (this is sort of what I
was trying to indicate here saying ``core level of code", but I didn't really
think through how necessary I think the marginalized version really is...).
... I don't think this has to be very ``cryptic" even under the hood though...
I wouldn't call a $\chi^2$ table in $\Delta^2_L, \neff(z=3,k=0.009 s/km)$ 
cryptic... } 

\afhid{Perfect. Just for the record, in my original text (two days ago) I never
meant to talk about the CosmoMC option, I always had in mind to compress it
all to a look-up table with $\Delta^2_L, \neff(z=3,k=0.009 s/km)$ or 
equivalent (may be extending to $f_\star$ or running.}

\pvmhid{Ok, since it isn't actually perfectly trivial to define 
$\Delta^2_L, \neff(z=3,k=0.009 s/km)$ in practice, e.g., do you really want to
do an infinitesimal derivative at exactly this point or some kind of broader
thing, I think you will always want to control how that is done by asking 
for full P(k), and they should be happy to let you take care of it, so maybe it
could get a little cryptic, but this is not much related to the more physical
stuff related to emulator.} 
\afhid{Agreed.}
\ashid{I agree as well.}


\subsection{From cosmological model to likelihood parameters}

We asked the user for the full linear density and velocity power, in
comoving coordinates and over a wide range of redshift and scales,
as well as the expansion history.
In this section we will describe how we compress all this information into
a handful of parameters that will be used to describe our likelihood.
We claim that this is an almost lost-less compression.

Note that this discussion is only relevant because we want to be able
to generate and publish the compressed likelihood, the look-up table that has
already marginalized over nuisance parameters.
If we only wanted to use the full likelihood, there would be no need to
do this compression.
For each model we could compute the required linear power at each redshift,
in velocity units, and ask the emulator for the flux power.

\subsubsection{Fiducial cosmological model}
 
We will choose a fiducial cosmological model, based on a recent Planck+BAO
analysis \cite{Planck2015}, and use it to compute a fiducial linear power
spectrum, for density $P^0(z,k)$ and velocity $P_\theta^0(z,k)$, and a
fiducial Hubble expansion, $H^0(z)$.
All quantities with a superscript $^0$ will refer to the fiducial model.

\subsubsection{Linear power shape}

We will assume that we can factorize the linear power spectrum in a 
constant shape (in comoving coordinates) and scale-independent growth 
around the central redshift, $z_\star=3$. 
In general we will use the subscript $_\star$ to refer to quantities that 
have been evaluated at $z_\star$.

The goal is then to compress the difference between the input power spectrum
at $z_\star$, $P_\star(k)$, and the fiducial power spectrum, $P_\star^0(k)$, 
into a handful of parameters.
It would be trivial to extend this analysis to include BAO information, but
for simplicity we will ignore the (small) BAO information in the simulations
and focus on the differences in the overall shape of the power spectra.
\begin{equation}
 B(k) = \frac{P_\star(k)}{P_\star^0(k)} ~. 
\end{equation}
We could use different parameterizations for $B(k)$, but we probably only
need 2-3 parameters.

\subsubsection{Linear growth}

The velocity power spectrum $P_\theta(z,k)$ is related to the linear density
power $P(z,k)$ by the logarithmic growth rate $f(z,k)$.

In the redshift and scales of interest, this growth rate is very close
to scale-independent, and we will only use the input velocity power spectrum
to compute an effective growth rate as a function of redshift, $f(z)$
\footnote{Note that this would be true even in cosmologies with relatively
massive neutrinos, as shown in \cite{Viel2010,Pedersen2018}.
\todo{Quantify}}.

This scale-independent growth rate $f(z)$ has two main effects: it sets
the redshift evolution of the linear power around $z_\star$, and it sets
the amount of velocity power in the model, that will contribute to the flux
power spectrum via redshift space distortions.

As we discuss in Appendix \ref{app:eq}, in relevant LCDM models, and at
$2 < z < 5$, the differences in growth rate are typically less than 1\%
(see Figure \ref{fig:fz_Om}).
To absorb this difference, we will add as a likelihood parameter the 
difference between the growth rate with that in the fiducial cosmology, 
$\Delta f_\star = f(z_\star) - f^0(z_\star)$, evaluated at $z_\star=3$. 
This compression is equivalent to assuming that the different growth rate
at $z_\star$ is enough to compute the difference in linear growth at any
redshift (within the range):
\begin{equation}\label{eq:growth}
 \frac{D(z)}{D^0(z)} = 1 + \Delta f_\star ~ \frac{\Delta z}{1 + z_\star} ~.
\end{equation} 
We discuss this approximation further in Appendix \ref{app:eq}.

\subsubsection{Hubble expansion}

If we could observe the \lya\ power spectrum in comoving coordinates, that 
would be enough. 
However, we observe the power spectrum in observing coordinates, wavelengths
and angles, and a more natural choice is to use velocity units ($\kms$) for 
the clustering measurements. 
Indeed, all recent measurements of the 1D \lya\ power reported their results
in units of $\kms$, and we will assume the same in this discussion.

In general, we would need to use $H(z)$ from each model to compare 
measurements in $\kms$ with model preditions in $\Mpc$:
\begin{equation}
 q = \frac{1+z}{H(z)} ~ k = a_v k~,
\end{equation}
where we use $q$ to refer to wavenumbers in velocity units, and we have 
defined $a_v$ as the transformation from $\kms$ to $\Mpc$.
This would force us to add in the emulator some sort of Hubble 
parameter, either at $z=0$ ($h$) or at $z_\star=3$. 
However, as suggested by \cite{McDonald2005a}, it is possible to avoid this
burden if we parameterize our likelihood (the linear power spectrum) already
in units of $\kms$. 

\ashid{This confused the shit out of me until very
  recently. I vote for having power spectra in Mpc and also require
  $H(z_\star)$ and do this conversion internally -- it is trivial. }
\afhid{Yes, \anze, that was the entire point of the user interface. 
We ask the users to provide power in $\Mpc$, and we ask for $H(z)$, 
and we do the convertions under the hood.}

We claim that two models with different expansion histories $H(z)$, but the
same linear power in units of $\kms$, will have very similar \lya\ power
spectra, with small remaining differences being caused by astrophysical 
effects (different reionization history, different thermal history, different
mean flux...). 
And since we plan to marginalize over these to get the final cosmological 
constraints, we do not need to worry about these differences. 
For the rest of this discussion, we will assume that this is true.

\pvmhid{I agree with \anze\ that you should move to just using Mpc and asking
for $H(z)$, where, a clincher is in 3D you can't get
away from asking for at least $(H D_A)(z)$. The argument about km/s was always
that you should quote results as a linear power measurement in km/s at 
$z\sim 3$, *if you are going to quote results from LyaF $\sim$alone* -- the
goal there was to produce ``model independent" constraints that could be
propagated forward.}
\afhid{My plan was to translate the flux power into linear power, in $\kms$, 
and make that public as a look-up table, with a user interface in $\Mpc$ to 
make it easier for others to use. 
This is what I was trying to explain all along, mostly in section
\ref{sec:over}, but I guess I didn't do a good job \smiley.} 
\pvmhid{But if 
you're going to be using flux power directly in global chains fitting 
cosmological parameters, which seems to be what you're talking about here, 
that argument doesn't apply -- you have the only relevant cosmological model 
on hand point-by-point. }
\afhid{I never talked about doing this! Only \anze\ talked about it.
I only talked about cosmological models as a wrapper, but the likelihood
and the emulator would only hear about our compressed parameterization.}
\pvmhid{In \anze's defense, it was a good point that you want to make sure
the mode with LyaF nuisance parameters symmetric with cosmological parameters
in CosmoMC is possible.}
\afhid{Of course! I learned a lot from this discussion the last week \smiley.}
\pvmhid{This does raise the question though, how much you 
want to commit to that approach, i.e., to primarily present a likelihood code
that does direct fits to flux power measurements on the fly, vs. boiling 
things down to linear power measurement (which you would again obviously 
present in observable coordinates). Certainly internally you want to be able
to do both, and the code to do linear boil-down should probably wrap the other
one, but there is a question where to spend more time cleaning and advertising. 
Arguments in favor of boil-down to linear could be speed (in old days, like 
I said, we could do both, but fit to flux power really slowed down global
fits, while my $\chi^2$ table gave identical results), and just... it is nice
to be able to show the more-or-less LyaF-model-independent cosmological thing
you claim to have measured, not just present a big black likelihood box. 
It helps, e.g., estimate how these constraints will affect new cosmological  
models. In any case, you can figure out what to advertise later. It seems like
first goal should be to produce $\chi^2$ contours for, e.g.,  
$\Delta^2,\neff(z=3, k=0.009 s/km)$,  
defined as deviations from a central model (i.e.,
effectively variations of $A_s$ and $n_s$ with other parameters fixed), 
to compare to past... this is clearly going to be *most of* what matters.} 
\afhid{Yes, I agree with the above.}

Therefore, we will use the expansion rate $H(z_\star)$ provided by the user
to compute $a_v$ and the linear power in velocity units at $z_\star=3$:
\begin{equation}
 \tilde P(q) = a_v^3 ~ P(z_\star, k= q / a_v) ~.
\end{equation}
where we use $\tilde P$ to refer to power spectra in units of velocity.
We can now redefine a term for the ratio of the power spectra, in velocity
units:
\begin{equation}
 \tilde B(q) = \frac{\tilde P(q)}{\tilde P^0(q)} 
  = \left(\frac{a_v}{a_v^0}\right)^3 
      \frac{P(z_\star, q / a_v)}{P^0(z_\star, q / a_v^0)} ~.
\end{equation}
We will use $\beta$ to refer to the (2 or 3) parameters describing the shape
of the linear power.

To sum up, all cosmological information will be compreseed to the following
parameters:
\begin{itemize}
 \item 2 or 3 $\beta$ parameters describing $\tilde B(q)$, the ratio of 
  the linear power at $z_\star=3$ with respect to the fiducial, in velocity
  units.
 \item $\Delta f_\star$, the difference of growth rates at $z_\star=3$.
\end{itemize}

\AFR{As discussed below, we *might* also add a parameter describing 
the (des-)acceleration of the Universe at $z_\star$, to describe the 
differences in expansion history between the cosmologies below and above
$z_\star$, similarly to what we have done with growth in equation 
\ref{eq:growth}.}

Note that in \cite{McDonald2005a} the main result was a look-up table with
only two $\beta$ parameters (amplitude and slope around $q = 0.009 \ikms$),
but they also discussed extensions with extra parameters: a third $\beta$ 
parameter (running) and a parameter describing the linear growth around 
$z_\star=3$, similar to the $\Delta f_\star$ presented here, for which
they found they did not have much constraining power.


\subsection{Constructing the compressed likelihood}

We want to compute a look-up table with the likelihood as a function of the
cosmologically relevant parameters $\theta$, describing the shape of the
linear power at $z_\star=3$ in velocity units ($\beta$) and the linear growth
($\Delta f_\star$).
There are many possible implementations, but in this section we will focus
on a simple one. 

We start by specifying a grid of values of the $\theta$ parameters
\footnote{We could probably get away with an irregular grid.
It would make interpolation a tiny bit more complicated, but it would help us
in setting up the simulations.}, and for each of these we will compute the
marginalized likelihood by integrating equation \ref{eq:marg}:
\begin{equation} 
 L(\vd | \theta)
  = \int d\phi ~ \Pi(\phi) ~ L(\vd | \theta, \phi) ~,
\end{equation}
where $\phi$ are the nuisance parameters (mean flux, temperature...), and 
$\Pi(\phi)$ are the priors on the nuisance parameters.
We will compute this integral using MCMC or similar. 

\ashid{Again, marginalisation is for the user to do. The code should
output total $\chi^2$ at this point and optionally data-points and
theory predictions at this point include full gory of nuissance
parameters. If you look at eg \texttt{cosmomc} it internally
marginalises already over some 15 Planck parameters, it can do 15
more for us.}
\afhid{Ok, this is the type of discussion I wanted to have. 
Is this really how it was done in SDSS-I? What was the look-up table for, then?
To make plots?}
\pvmhid{I guess I should further to un-agree with \anze... 
If you wanted to assume everyone
will be using cosmoMC so you could just make a module for that and broadcast
it, I guess it would be ok (but I think still a lot of people would consider
your LyaF parameters to be too literally a nuisance... and be less tolerant 
of it than of Planck nuisance), but I don't think you want to think this way.
I don't think one MCMC code will dominate, so you need to assume people are
going to be grafting your likelihood code into various things themselves, 
so you want to make sure to have a very easy option. }
\afhid{Agreed. We should have an easy version for all to use, and a difficult
one for testing and for the experts to use.}

In order to compute the full likelihood $L(\vd | \theta, \phi)$ we will
need an \textit{emulator} that will use whatever simulations are available
to make a prediction for the flux power in a particular model.
However, we will not pass the parameters $(\theta,\phi)$ to the emulator
and ask for a prediction for all data points $P_{\rm 1D}(z,k_\parallel)$.
Instead, we will do the following:
\begin{itemize}
 \item For each redshift $z_i$, we use the nuisance parameters $\phi$ to
  make a prediction for the mean flux at the redshift ($\bar F_i$), the
  values of the temperature-density relation at the redshit 
  ($T_{0i}$,$\gamma_i$) and the filtering scale at the redshift ($k_{Fi}$).
  $T_0$ sets a thermal broadening length $\sigma_0$, in velocity units,
  that can be used as a parameter instead.
  The filtering length, $k_F$, is discussed in more detail in the next
  section, but it is also naturally described in velocity units.
 \item At the same time, we use the parameters describing the linear power
  $\theta$ and the fiducial cosmology to make a prediction for the linear
  power in velocity units at the redshift ($\tilde P_i(q)$) and for the
  logarithmic growth rate at the redshift ($f_i$).
  How exactly this is done is described in Appendix \ref{app:P_L}.
 \item We then ask the emulator to give us a prediction for the flux power
  spectrum corresponding to 
  $\left( \bar F_i,~T_{0i},~\gamma_i,~k_{Fi},~\tilde P_i(q),~f_i \right)$.
  Note that we do not tell the emulator what redshift this corresponds to,
  and we do not tell it either anything about the $\theta$ or $\phi$
  parameters.
\end{itemize}

\afhid{This is one of the pieces that is still not crystal clear in my head.
Would we just take $\tilde P_L(z_\star,q)$, the linear power in velocity
units at the central redshift, and rescale it using the discussion around
equation \ref{eq:growth}?
If we did that, we would be missing the fact that the transformation between
comoving and velocity separations $a_v$ changes with redshift.
I guess one could use the fiducial model to compute this difference?
The alternative would be to have a 6th cosmological parameter describing
the difference in the change in the Hubble expansion around $z_\star$?}
\ashid{If DE is truly negligible for sensible models, then just don't worry
about it and assumed EdS. If it makes small corrections, then the best course
of action would be to also specify $d H^2/da=-3\Omega_m/a^4$ at $z=z_\star$.}
\pvmhid{I think there is a key thing you (Andreu) should add to your thinking
about these things: don't focus on the parameters you put in when running the
simulation, focus on the effective parameters you *achieve* for each redshift
output, i.e., the numbers you can associate most directly with the flux power
spectrum produced from that redshift output. 
E.g., there is a linear power spectrum associated with each redshift
output, which you can easily compute using CLASS -- you don't really care
where it came from in terms or evolution from higher z, or, if you do, it is
only as a very subdominant correction.}
\afhid{Yes, I got that. That's what I was trying to say here...}
\pvmhid{Even if you decided you needed to 
track differences in evolution for fixed output-time linear power, you would
probably want to do that by extracting $dP_{lin}/dz(z_{output})$ 
from CLASS, i.e., 
keep everything you associate with an output local in $z$. Going on, there is
an $F(z_{out})$, there is a $T(z_{out})$, there is a $k_F(z_{out})$ -- 
there is no
need to talk about a ``fiducial z" at all at this level. You only need to think
about that at a higher level of fitting, when, e.g., you want to produce 
$\chi^2$ contours in $\Delta^2_L(z=3), \neff(z=3)$ plane (fixing linear power
at other z assuming some model), or you want to enforce physically reasonable
temperature evolution connecting $T(z)$, $k_F(z)$. This isn't an entirely 
non-trivial attitude. E.g., if you didn't think you could summarize pressure
by a $k_F(z)$ you could calculate for each output, maybe you'd want to 
associate a full temperature history with each output instead of only 
writing down local-in-z quantities, and then you might want to parameterize
that thermal history somehow, but I would worry about that only when pushed to
it (and probably it would always be better to invent some local-in-z quantity
you could compute to capture the physical effect you were missing). 
To put it another way: you want to separate your picture into things you can
calculate about the conditions in the Universe at a given z without sims 
(including if necessary derivatives) -- you want to take advantage of these
kinds of things
as much as possible -- and then a simulation mapping of those
things into non-linear power (in more or less arbitrary units, followed by 
observation, applying the necessary units -- this part I think is easy for
everyone to agree on). }
\afhid{Yes, that was my plan.}
\pvmhid{This is an opportunity to say something I've been thinking about all
this including neutrino, etc., sim testing: by *far* the most efficient way to 
test whether an idea you have for simplifying the emulator parameterization is
good enough is
to just do the simple version and then see if it works in the case you think it
might not. Probably it will work, and if not you haven't lost anything since
you should just need to expand parameter space a little and add some sims to 
probe the new effect, still using what you have done (assuming it was sensible
and the addition is more or less perturbative).  }

\afhid{Pat, that is what I was trying to write... See discussion below, about
computing some quantities at the particular redshift, and then completely 
dropping the redshift altogether.}

