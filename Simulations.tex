\section{Simulations}
\label{sec:sims}

Describe here the simulations, include the initial conditions code (GenIC), 
the code to evolve the fields (MP-Gadget), the different boxes used, 
and the code to extract skewers (fake\_spectra).

Discuss also here the optical depth rescaling, different transfer functions
(if any), and thermal history effects (that will be mostly ignored in this
paper).


\subsection{Rescaling of the optical depth}

From each simulation we will get a set of snapshots, outputs at different 
redshifts. 

From each snapshot, we will extract \lya\ skewers, and use these to compute 
their power spectra. 
We will repeat this exercise for different rescalings of the optical depth, 
i.e., we will multiply the optical depth in all cells by a constant factor
in the range $0.8 < A_\tau < 1.2$ (approximately), and for each value of 
$A_\tau$ we will compute the transmitted flux fraction $F$, its mean value
(mean flux), and the power spectra of their fluctuations $\delta_F$. 

Therefore, from each snapshot we will get a set of power spectra for different
values of $A_\tau$.
We can label the different power spectra by their associated value of $A_\tau$,
or we can label them by their resulting value of the mean flux $\bar F$. 
\cite{Lukic2015} showed that the rescaling might introduce biases in the 
1D power spectrum for values of $A_\tau$ very different than one. 
However, their test compared two simulations with different thermal history
and different pressure, so it is difficult to tell whether the bias came 
from the rescaling or from the different IGM physics.
\AFR{It would be great to repeat this exercise in two simulations that 
have very similar thermal history but different mean flux, I will ask 
Jose Onorbe for help (he is visiting UCL soon). 
It is also possible that the test is clearer if we look at the 3D power,
where pressure only affects the high-k limit, and the scale independent
linera bias.}
\pvm{It seems useful to be sure to understand the physics you're talking about
here. If I am remembering correctly, a change in ionizing background at the
output time will change the ionization rate exactly proportionally, which 
will change the neutral density and therefor optical depth exactly 
proportionally, in the 
ionization equilibrium, fully ionized, only photo-ionization limit. 
So to say the rescaling ``doesn't work" is to say you have a breakdown 
somewhere in here. E.g., collisional ionization violates 
``only photo-ionization", but generally only at very high density, where...
if this was what was going on you really need to find a way to desensitize 
yourself to this high density, because surely you are getting it wrong in 
other ways. Basically I think this is the story with all possibilities, i.e.,
there is no possibility that the solution to any imperfection in this scaling
would be to not do the rescaling, because there is *nothing you can simulate
reliably* that can violate it. Much more likely (than collisional ionization)
of course, like you say here, a
change someone finds when they rerun with different ionizing background is
from changing temperatures, which you are supposed to be marginalizing over. 
}
\AFR{Yes, you are right. I had a chat with Jose about this a couple of weeks
ago, and he mentioned collisional ionization as a possible reason for the 
breakdown, but as you say we don't trust these high-density regions anyway.
It is more likely that the reason that \cite{Lukic2015} finds a "bias" is that
they have different thermal histories in the two simulations (they acknowledge
this is the case).
I will rewrite this section to make this more clear.}


\subsection{Rescaling of the temperature}

The Temperature-Density Relation (TDR) in the \lya\ forest can be reasonably
well described by a power law, 
\begin{equation}
 T(\rho) = T_0 \left(\frac{\rho}{\rho_0}\right)^{\gamma-1} ~,
\end{equation}
with a typical values for $\gamma$ between 1 and 1.6. 
If we use $\rho_0 = \bar \rho$, $T_0$ varies between 10,000 and 20,000K
\cite{Lukic2015}.

We can change the thermal history of the simulations by running the same box
with different \textit{TREECOOL} files, that contain the redshift evolution
of different heating and ionizing rates.

For each snapshot, we can fit their values of $T_0$ and $\gamma$, and use 
these to label the snapshot, instead of using the name of the TREECOOL, 
or the parameters that we have used to modify a given file 
(MP-Gadget can implement the recipes in \cite{Bolton2008} to modify TRECOOL
files by setting the parameters \textit{HeatAmplitude} and \textit{HeatSlope}).

Just like we did with the optical depth, we can rescale the temperatures in 
post-processing. 
This would capture correctly two different physical effects:
\begin{itemize}
 \item Thermal broadening: when extracting the skewers from the boxes, 
  the last step is to compute the redshift-space-distorted optical depth. 
  To do that, the temperature at each cell is used to decide the local 
  smoothing that we need to apply, what is known as the thermal broadening. 
  It is trivial to take the same optical depth skewer, and convolve it for
  different temperature rescalings.
 \item Recombination rates: the temperature also sets the recombination rate,
  $\alpha(T) \sim T^{-0.7}$. 
  We could recompute the recombination rate at each cell, 
  propagate this into a change in the neutral fraction (proportional to the 
  recombination rate), and finally to the optical depth (proportional to the 
  neutral fraction).
 \pvmhid{Note that these two are both essentially instantaneous, i.e., I don't 
think there is any physical possibility of them not using the same 
temperatures... (so I wouldn't mess with it, except maybe in some pedagogical
example)}
 \afhid{Great, thanks.}
\end{itemize}


\subsection{Adding pressure smoothing}

The small scale structure is suppressed on very small scales because of the
pressure in the gas.
As described in \cite{Hui1997,Gnedin1998}, the smoothing can be described
by a characteristic scale, $k_F(z)$, the \textit{filtering scale}, that is
an integral version of the Jeans length that depends also on the temperature
in the past.

It is important to distinguish the effect of pressure from the effect of
temperature, since one can have two snapshots with very similar temperature
but very different values of $k_F$ because of a different reionization
history.

\pvmhid{Which is of course not really the same temperature (unlike above), i.e.,
the functional derivative of $P(z)$ with respect to $T(z')$ through the effect
of pressure is zero at $z=z'$, while it is a delta function through thermal
broadening and recombination rate... I know you know this, this is just a
different way to say it... if all pressure came from temperature in observable
range I'd say this is a little bit of a nitpicky distinction, but the
contribution from pressure at early times sort of fundamentally decouples the
two uses of temperature...}
\afhid{Agreed.}

It would be interesting to see if we can add smoothing in post-processing,
effectively lowering the value of $k_F$ in the simulation by hand.
Of course, it would not be possible to reduce the smoothing, and to do that
one would need to run a simulation with an earlier redshift of reionization.

\AFR{We could also ask Jose Onorbe for help to setup this type of test. 
We would also need to discuss the best way to measure $k_F$ in the snapshots:
fit a Gaussian kernel in the power spectrum of $F_{\rm real}$, where no 
redshift-space distortions have been included? I believe that is what is 
used in all papers by Hennawi / Lukic / Onorbe.}
\as{I remember having massive trouble with this when Nishi was still
  around. I naively imagined that I would just plot the 3D baryon
  power spectrum, see supression by eye and call $k$ at the midpoint of
  supression the smoothing. No cigar -- the baryon power spectrum is
  dominated by shot-noise of baryons in halos, you really need to be a
  bit clever about this. Perhaps this has been solved.}


\subsection{Labelling the snapshots}

In the linear regime, and for a single specie, the growth of structure is
scale independent, and it can be described by the growth factor $D(z)$.
%\begin{equation}
% P_L(z,k) = \frac{D^2(z)}{D^2(z_0)} ~ P_L(z_0,k) ~.
%\end{equation}
If the \lya\ power spectra depended only on the linear power spectrum, this
would suggest that there would be a complete degeneracy between changing
the overall amplitude of the linear power spectrum and changing the redshift
at which we ouput the snapshot.
Therefore, we could use the amplitude of the linear power at a given snapshot
to label it, and we could potentially use different snapshots of the same
simulation to study models with different amplitudes of the linear power.

\afhid{How do we measure the linear power in the snapshot?
I could see three options (in order of my preference):
predict it using the power measured in the initial conditions, and the relative
growth as computed from CAMB/CLASS;
measure the density power in the snapshot, and fit the growth factor from
the low-k part;
run a very cheap simulation without hydro and a very low value of $A_s$, to
compute the actual linear power in the simulation (that might sadly differ
from the predicted by CAMB/CLASS because of issues in the linear growht).}
\ashid{By far preferrable, in fact the only way that wouldn't look dodgy
would be not to measure it at all \smiley . It is a known quantity give
linear codes \smiley .}
\afhid{Pat always claims that measured power is better than predicted power,
given cosmic variance in the box.
We shouldn't care about the mean power over infinite number of realizations,
it is more useful to talk about the actual power than went into the simulation,
and that is why we could measure linear power in the initial
conditions.}
\ashid{Ok, but then you have the IC power spectrum that you
can multiply by the correct transfer function.}

To sum up, each snapshot will be used to generate multiple simulated
fields in post-process (rescaling optical depth, temperature, pressure...),
and we will compute the power spectrum for each of the simulated fields.
We will also compute (using CAMB/CLASS) the predited linear power for the
snapshot (in velocity units) and its velocity power, and will compute other
IGM properties like the mean flux $\bar F$, the TDR parameters
($T_0$ and $\gamma$) and the filtering scale $k_F$.
\ashid{Perhaps you need $f$ again.}
\afhid{Yes, good point.}

The emulator will have a list of all the models for which we have simulated
power spectra, and it will specify a metric quantifying the separation
between two models.
Every time we ask for a the prediction for a given model (specified by
$P(q)$, $P_\theta(q)$, $\bar F$, $T_0$ $\gamma$, and $k_F$), the emulator
will use the nearest points and (somehow) interpolate between these.

Redshift will NOT be a label describing the simulated field, and neither
will be the TREECOOL file or the redshift of reionization.
Since we will define the linear power in units of $\kms$, we will not 
need to include the Hubble parameter at the box as a label.
We will not care about any other cosmological parameter in the box
either.
