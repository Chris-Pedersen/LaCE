\section{The emulator} \label{sec:emu}

For a given combination of linear power parameters $\theta$, i.e., for
each point in the lookup table, we want to compute the integral in
equation \ref{eq:marg}.
To do this, we need to specify priors $\Pi(\phi)$, but more importantly
we need to compute the likelihood $L(\vd | \theta, \phi)$.

We need, therefore, to make predictions for the 1D flux power spectrum
at a certain set of points, $P_{1D}(z,q)$, in velocity units, given a model
defined by ($\theta$,$\phi$).
We discuss the (nuisance) astrophysical parameters $\phi$ in more detail
later on, but for now we will assume that there are only two parameters:
an overall normalization of the logarithm of the mean flux $\ln{\bar F_0}$,
multiplying some fiducial redshift evolution, and an overall normalization
of the filtering length $k_{F 0}$ (associated to the smoothing of the gas),
also multiplying a fiducial redshift evolution. 


\subsection{Emulating a particular redshift}

For each redshfit $z_i$, we compute the corresponding value of the mean flux
($\bar F_i$) and filtering length ($k_{Fi}$).

Using the linear power parameters ($\theta$), and the fiducial
power spectrum (in velocity units), we are able to compute the expected
linear power for this particular model, in velocity units, at this redshift,
$\tilde P_i(q)$.

\AFR{This is one of the pieces that is still not crystal clear in my head.
Would we just take $\tilde P_L(z_\star,q)$, the linear power in velocity
units at the central redshift, and rescale it using the discussion around
equation \ref{eq:growth}?
If we did that, we would be missing the fact that the transformation between
comoving and velocity separations $a_v$ changes with redshift.
I guess one could use the fiducial model to compute this difference?
The alternative would be to have a 6th cosmological parameter describing
the difference in the change in the Hubble expansion around $z_\star$?}
\as{If DE is truly negligible for sensible models, then just don't
  worry about it and assumed EdS. If it makes small corrections, then
  the best course of action would be to also specify $d
  H^2/da=-3\Omega_m/a^4$ at $z=z_\star$.}
\pvm{I think there is a key thing you (Andreu) should add to your thinking 
about these things: don't focus on the parameters you put in when running the
simulation, focus on the effective parameters you *achieve* for each redshift 
output, i.e., the numbers you can associate most directly with the flux power 
spectrum produced from that redshift output. 
E.g., there is a linear power spectrum associated with each redshift
output, which you can easily compute using CLASS -- you don't really care
where it came from in terms or evolution from higher z, or, if you do, it is 
only has a very subdominant correction. Even if you decided you needed to 
track differences in evolution for fixed output-time linear power, you would
probably want to do that by extracting $dP_{lin}/dz(z_{output})$ 
from CLASS, i.e., 
keep everything you associate with an output local in $z$. Going on, there is
an $F(z_{out})$, there is a $T(z_{out})$, there is a $k_F(z_{out})$ -- 
there is no
need to talk about a ``fiducial z" at all at this level. You only need to think
about that at a higher level of fitting, when, e.g., you want to produce 
$\chi^2$ contours in $\Delta^2_L(z=3), \neff(z=3)$ plane (fixing linear power
at other z assuming some model), or you want to enforce physically reasonable
temperature evolution connecting $T(z)$, $k_F(z)$. This isn't an entirely 
non-trivial attitude. E.g., if you didn't think you could summarize pressure
by a $k_F(z)$ you could calculate for each output, maybe you'd want to 
associate a full temperature history with each output instead of only 
writing down local-in-z quantities, and then you might want to parameterize
that thermal history somehow, but I would worry about that only when pushed to
it (and probably it would always be better to invent some local-in-z quantity
you could compute to capture the physical effect you were missing). 
To put it another way: you want to separate your picture into things you can
calculate about the conditions in the Universe at a given z without sims 
(including if necessary derivatives) -- you want to take advantage of these
kinds of things
as much as possible -- and then a simulation mapping of those
things into non-linear power (in more or less arbitrary units, followed by 
observation, applying the necessary units -- this part I think is easy for
everyone to agree on).
This is an opportunity to say something I've been thinking about all this 
including neutrino, etc., sim testing: by *far* the most efficient way to 
test whether an idea you have for simplifying the emulator parameterization is
to just do the simple version and then see if it works in the case you think it
might not. Probably it will work, and if not you haven't lost anything since
you should just need to expand parameter space a little and add some sims to 
probe the new effect, still using what you have done (assuming it was sensible
and the addition is more or less perturbative).  }


At this point, we can completely forget about the redshift $z_i$.

Instead, we will go to our simulation database, and ask:
is there any simulated flux power spectra that was computed from a snapshot
with similar linear power (in velocity units) $\tilde P_i(q)$, similar
mean flux $\bar F_i$ and similar filtering length $k_{Fi}$? \as{And
  similar $f$ for velocity effects? Again, only matters if non-EdS matters.}
\pvm{Remember that much of non-EdS effects can still be accounted for by just
linear theory. Thinking of neutrinos, I think it is really best to get away
from talking about $f(z)$, which is not well-defined when it is really 
$f(z,k)$. 
Remember that you can easily compute from CLASS the velocity power spectrum as
well as density, which gives you your leading order handle on changes in 
evolution... (arguably if you had to choose you'd probably want this instead
of density power for LyaF, but you don't have to choose...)}


If we had an extremely large number of simulations, we could just setup a
metric to find the closest snapshot, and directly read the flux power
from the snapshot.
Since we will have a sparse sampling of the parameter space, we will need
to do some interpolation between them. \as{This is a good way to think
  about this, yes.}
This interpolation is precisely the role of the \textit{emulator}.

Note that the internal metric used for the interpolation does not need to 
use the same parameters $\theta$ describing the linear power.

