\section{The emulator} \label{sec:emu}

As we discussed in the previous section, in order to evaluate the
likelihood of a given dataset we ask the emulator to provide the predicted
flux power spectrum $P_{\rm 1D}(z_i,k_\parallel)$ for a given model,
specified by 
$\left( \bar F_i,~T_{0i},~\gamma_i,~k_{Fi},~\tilde P_i(q),~f_i \right)$.
Note, again, that the emulator does not need to know about the concept of
redshift.
Yes, snapshots from simulations have associated output redshifts, but this
information does not need to be passed to the emulator.

\ashid{And similar $f$ for velocity effects? Again, only matters if non-EdS
matters.}
\pvmhid{Remember that much of non-EdS effects can still be accounted for by
just linear theory.} 
\ashid{But it changes growth, which in turn changes velocity smoothing. 
So yes, linear power spectrum but also most likely its time derivative (or
equivalently velocity power spectrum)} 
\pvmhid{Thinking of neutrinos, I think it is really best to get away from
talking about $f(z)$, which is not well-defined when it is really $f(z,k)$.
Remember that you can easily compute from CLASS the velocity power spectrum as
well as density, which gives you your leading order handle on changes in 
evolution... (arguably if you had to choose you'd probably want this instead
of density power for LyaF, but you don't have to choose...)}
\afrhid{Yes, we could add linear velocity power instead of $f(z)$, and compute
from there any parameter we want to use internally.}

If we had an extremely large number of simulations, we could just setup a
metric to find the closest snapshot, and directly read the flux power
from the snapshot.
Since we will have a sparse sampling of the parameter space, we will need
to do some interpolation between them. 
\ashid{This is a good way to think about this, yes.}
This interpolation is precisely the role of the \textit{emulator}.
There are different options for the emulator itself, for instance one could
use Gaussian Processes \cite{Heitmann2009,Heitmann2014,SLAC2018,
Walther2018a,Bird2018,Rogers2018c}, but the exact interpolation scheme will
not be discussed futher in this paper.

In order to reduce the cosmic variance noise in the simulated flux power,
we could decide to emulate instead the ratio of the flux power with respect
to the flux power in the fiducial model, and use the same random seed in all
simulations.
However, in different cosmologies we will have the same fluctuations in the
initial conditions (defined in comoving coordinates) mapped into different 
noise spikes in the band powers (defined in velocity units).
We could avoid this by using different box sizes for different cosmologies,
so that they all have the same box size in velocity units at $z_\star=3$.
Even then, we would only match the noise spikes at one redshift, and there
could be (very minor) differences at other redshifts that could confuse
the emulator.

\afr{Actually, we could avoid this nuisance by letting the emulator work
in comoving coordinates, in $\Mpc$.
We could use the expansion history from the fiducial model $H^0(z)$ to
convert all quantities back and forth when needed.
The only important thing is that if we use $\Mpc$ for the linear power
spectrum and for the flux power spectrum, we have to use $\Mpc$ for the
thermal broadening at $T_0$, $\sigma_0$, and for the filtering scale $k_F$.
In any case, it would not affect the setting of the simulations, only the
internal interpolation of the emulator, and we could try and compare
both approaches.}

\pvmhid{This seems outdated, in that if you say from the beginning you are going
to internally define boxes and bands and things fixed in comoving Mpc, 
modes will naturally always align between redshifts and models, 
without even needing to think about it... and for this reason it seems like 
the thing to plan to do...}

In order to reduce the effect of cosmic variance and noise spikes, we could
also decide to run \textit{paired-fixed} simulations 
\cite{Pontzen2016,Angulo2016,Villaescusa2018,Anderson2018}.

Finally, we could decide to fit the simulated flux power with a handful of
coefficients (polynomials or PCA components), and interpolate these
instead of the noisy band powers.

\afr{Written this way, it is clear that we are talking about emulating
the theory, and not emulating the likelihood.
This has not been clear to me in the past...
One of the key features of GP emulators is that they can provide an
estimate on the uncertainty in the theoretical prediction.
In \cite{Rogers2018c} we have been looking at how to implement a
\textit{refinement} step, where we combine the posterior probability for
(a very different) set of parameters with the predicted theoretical
uncertainty to decide where to run the next batch of simulations to more
efficiently improve the description of the posterior.
I wonder how could one implement something similar in the setting described
here.}

\afr{I guess there are two things that should be addressed:
How do we propagate the theoretical uncertainty reported by the emulator
to uncertainties in the likelihood?
And once we identify the points in parameter space that we would like to
refine, specified by ($\theta$,$\phi$), how do we translate this into a
configuration for the simulation to run, specified by a linear power spectrum
at $z_{\rm ini}=99$, a cosmology to evolve the density fields, and a TRECOOL
file specifying the heating and ionization rates?
I am sure there are approximated ways to do this, but we'd need to think
about it.}
